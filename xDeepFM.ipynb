{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import abc\n",
    "import time\n",
    "import six\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "import yaml\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_metric(labels, preds, metrics):\n",
    "    \"\"\"Calculate metrics,such as auc, logloss.\n",
    "    \n",
    "    FIXME: \n",
    "        refactor this with the reco metrics and make it explicit.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    for metric in metrics:\n",
    "        if metric == \"auc\":\n",
    "            auc = roc_auc_score(np.asarray(labels), np.asarray(preds))\n",
    "            res[\"auc\"] = round(auc, 4)\n",
    "        elif metric == \"rmse\":\n",
    "            rmse = mean_squared_error(np.asarray(labels), np.asarray(preds))\n",
    "            res[\"rmse\"] = np.sqrt(round(rmse, 4))\n",
    "        elif metric == \"logloss\":\n",
    "            # avoid logloss nan\n",
    "            preds = [max(min(p, 1.0 - 10e-12), 10e-12) for p in preds]\n",
    "            logloss = log_loss(np.asarray(labels), np.asarray(preds))\n",
    "            res[\"logloss\"] = round(logloss, 4)\n",
    "        elif metric == \"acc\":\n",
    "            pred = np.asarray(preds)\n",
    "            pred[pred >= 0.5] = 1\n",
    "            pred[pred < 0.5] = 0\n",
    "            acc = accuracy_score(np.asarray(labels), pred)\n",
    "            res[\"acc\"] = round(acc, 4)\n",
    "        elif metric == \"f1\":\n",
    "            pred = np.asarray(preds)\n",
    "            pred[pred >= 0.5] = 1\n",
    "            pred[pred < 0.5] = 0\n",
    "            f1 = f1_score(np.asarray(labels), pred)\n",
    "            res[\"f1\"] = round(f1, 4)\n",
    "        else:\n",
    "            raise ValueError(\"not define this metric {0}\".format(metric))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "__all__ = [\"BaseModel\"]\n",
    "class BaseModel:\n",
    "    def __init__(self, hparams, iterator_creator, graph=None, seed=None):\n",
    "        \"\"\"Initializing the model. Create common logics which are needed by all deeprec models, such as loss function, \n",
    "        parameter set.\n",
    "        Args:\n",
    "            hparams (obj): A tf.contrib.training.HParams object, hold the entire set of hyperparameters.\n",
    "            iterator_creator (obj): An iterator to load the data.\n",
    "            graph (obj): An optional graph.\n",
    "            seed (int): Random seed.\n",
    "        \"\"\"\n",
    "        self.seed = seed\n",
    "        tf.set_random_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.graph = graph if graph is not None else tf.Graph()\n",
    "        self.iterator = iterator_creator(hparams, self.graph)\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.hparams = hparams\n",
    "\n",
    "            self.layer_params = []\n",
    "            self.embed_params = []\n",
    "            self.cross_params = []\n",
    "            self.layer_keeps = tf.placeholder(tf.float32, name=\"layer_keeps\")\n",
    "            self.keep_prob_train = None\n",
    "            self.keep_prob_test = None\n",
    "            self.is_train_stage = tf.placeholder(tf.bool, shape=(), name=\"is_training\")\n",
    "\n",
    "            self.initializer = self._get_initializer()\n",
    "\n",
    "            self.logit = self._build_graph()\n",
    "            self.pred = self._get_pred(self.logit, self.hparams.method)\n",
    "\n",
    "            self.loss = self._get_loss()\n",
    "            self.saver = tf.train.Saver(max_to_keep=self.hparams.epochs)\n",
    "            self.update = self._build_train_opt()\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            self.merged = self._add_summaries()\n",
    "\n",
    "        # set GPU use with demand growth\n",
    "        gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "        self.sess = tf.Session(\n",
    "            graph=self.graph, config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "        )\n",
    "        self.sess.run(self.init_op)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _build_graph(self):\n",
    "        \"\"\"Subclass will implement this.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_loss(self):\n",
    "        \"\"\"Make loss function, consists of data loss and regularization loss\n",
    "        \n",
    "        Returns:\n",
    "            obj: Loss value\n",
    "        \"\"\"\n",
    "        self.data_loss = self._compute_data_loss()\n",
    "        self.regular_loss = self._compute_regular_loss()\n",
    "        self.loss = tf.add(self.data_loss, self.regular_loss)\n",
    "        return self.loss\n",
    "\n",
    "    def _get_pred(self, logit, task):\n",
    "        \"\"\"Make final output as prediction score, according to different tasks.\n",
    "        \n",
    "        Args:\n",
    "            logit (obj): Base prediction value.\n",
    "            task (str): A task (values: regression/classification)\n",
    "        \n",
    "        Returns:\n",
    "            obj: Transformed score\n",
    "        \"\"\"\n",
    "        if task == \"regression\":\n",
    "            pred = tf.identity(logit)\n",
    "        elif task == \"classification\":\n",
    "            pred = tf.sigmoid(logit)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"method must be regression or classification, but now is {0}\".format(\n",
    "                    task\n",
    "                )\n",
    "            )\n",
    "        return pred\n",
    "\n",
    "    def _add_summaries(self):\n",
    "        tf.summary.scalar(\"data_loss\", self.data_loss)\n",
    "        tf.summary.scalar(\"regular_loss\", self.regular_loss)\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        merged = tf.summary.merge_all()\n",
    "        return merged\n",
    "\n",
    "    def _l2_loss(self):\n",
    "        l2_loss = tf.zeros([1], dtype=tf.float32)\n",
    "        # embedding_layer l2 loss\n",
    "        for param in self.embed_params:\n",
    "            l2_loss = tf.add(\n",
    "                l2_loss, tf.multiply(self.hparams.embed_l2, tf.nn.l2_loss(param))\n",
    "            )\n",
    "        params = self.layer_params\n",
    "        for param in params:\n",
    "            l2_loss = tf.add(\n",
    "                l2_loss, tf.multiply(self.hparams.layer_l2, tf.nn.l2_loss(param))\n",
    "            )\n",
    "        return l2_loss\n",
    "\n",
    "    def _l1_loss(self):\n",
    "        l1_loss = tf.zeros([1], dtype=tf.float32)\n",
    "        # embedding_layer l2 loss\n",
    "        for param in self.embed_params:\n",
    "            l1_loss = tf.add(\n",
    "                l1_loss, tf.multiply(self.hparams.embed_l1, tf.norm(param, ord=1))\n",
    "            )\n",
    "        params = self.layer_params\n",
    "        for param in params:\n",
    "            l1_loss = tf.add(\n",
    "                l1_loss, tf.multiply(self.hparams.layer_l1, tf.norm(param, ord=1))\n",
    "            )\n",
    "        return l1_loss\n",
    "\n",
    "    def _cross_l_loss(self):\n",
    "        \"\"\"Construct L1-norm and L2-norm on cross network parameters for loss function.\n",
    "        Returns:\n",
    "            obj: Regular loss value on cross network parameters.\n",
    "        \"\"\"\n",
    "        cross_l_loss = tf.zeros([1], dtype=tf.float32)\n",
    "        for param in self.cross_params:\n",
    "            cross_l_loss = tf.add(\n",
    "                cross_l_loss, tf.multiply(self.hparams.cross_l1, tf.norm(param, ord=1))\n",
    "            )\n",
    "            cross_l_loss = tf.add(\n",
    "                cross_l_loss, tf.multiply(self.hparams.cross_l2, tf.norm(param, ord=2))\n",
    "            )\n",
    "        return cross_l_loss\n",
    "\n",
    "    def _get_initializer(self):\n",
    "        if self.hparams.init_method == \"tnormal\":\n",
    "            return tf.truncated_normal_initializer(stddev=self.hparams.init_value, seed=self.seed)\n",
    "        elif self.hparams.init_method == \"uniform\":\n",
    "            return tf.random_uniform_initializer(\n",
    "                -self.hparams.init_value, self.hparams.init_value, seed=self.seed\n",
    "            )\n",
    "        elif self.hparams.init_method == \"normal\":\n",
    "            return tf.random_normal_initializer(stddev=self.hparams.init_value, seed=self.seed)\n",
    "        elif self.hparams.init_method == \"xavier_normal\":\n",
    "            return tf.contrib.layers.xavier_initializer(uniform=False, seed=self.seed)\n",
    "        elif self.hparams.init_method == \"xavier_uniform\":\n",
    "            return tf.contrib.layers.xavier_initializer(uniform=True, seed=self.seed)\n",
    "        elif self.hparams.init_method == \"he_normal\":\n",
    "            return tf.contrib.layers.variance_scaling_initializer(\n",
    "                factor=2.0, mode=\"FAN_IN\", uniform=False, seed=self.seed\n",
    "            )\n",
    "        elif self.hparams.init_method == \"he_uniform\":\n",
    "            return tf.contrib.layers.variance_scaling_initializer(\n",
    "                factor=2.0, mode=\"FAN_IN\", uniform=True, seed=self.seed\n",
    "            )\n",
    "        else:\n",
    "            return tf.truncated_normal_initializer(stddev=self.hparams.init_value, seed=self.seed)\n",
    "\n",
    "    def _compute_data_loss(self):\n",
    "        if self.hparams.loss == \"cross_entropy_loss\":\n",
    "            data_loss = tf.reduce_mean(\n",
    "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "                    logits=tf.reshape(self.logit, [-1]),\n",
    "                    labels=tf.reshape(self.iterator.labels, [-1]),\n",
    "                )\n",
    "            )\n",
    "        elif self.hparams.loss == \"square_loss\":\n",
    "            data_loss = tf.sqrt(\n",
    "                tf.reduce_mean(\n",
    "                    tf.squared_difference(\n",
    "                        tf.reshape(self.pred, [-1]),\n",
    "                        tf.reshape(self.iterator.labels, [-1]),\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        elif self.hparams.loss == \"log_loss\":\n",
    "            data_loss = tf.reduce_mean(\n",
    "                tf.losses.log_loss(\n",
    "                    predictions=tf.reshape(self.pred, [-1]),\n",
    "                    labels=tf.reshape(self.iterator.labels, [-1]),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"this loss not defined {0}\".format(self.hparams.loss))\n",
    "        return data_loss\n",
    "\n",
    "    def _compute_regular_loss(self):\n",
    "        \"\"\"Construct regular loss. Usually it's comprised of l1 and l2 norm.\n",
    "        Users can designate which norm to be included via config file.\n",
    "        Returns:\n",
    "            obj: Regular loss.\n",
    "        \"\"\"\n",
    "        regular_loss = self._l2_loss() + self._l1_loss() + self._cross_l_loss()\n",
    "        return tf.reduce_sum(regular_loss)\n",
    "\n",
    "    def _train_opt(self):\n",
    "        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "        Returns:\n",
    "            obj: An optimizer.\n",
    "        \"\"\"\n",
    "        lr = self.hparams.learning_rate\n",
    "        optimizer = self.hparams.optimizer\n",
    "\n",
    "        if optimizer == \"adadelta\":\n",
    "            train_step = tf.train.AdadeltaOptimizer(lr)\n",
    "        elif optimizer == \"adagrad\":\n",
    "            train_step = tf.train.AdagradOptimizer(lr)\n",
    "        elif optimizer == \"sgd\":\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr)\n",
    "        elif optimizer == \"adam\":\n",
    "            train_step = tf.train.AdamOptimizer(lr)\n",
    "        elif optimizer == \"ftrl\":\n",
    "            train_step = tf.train.FtrlOptimizer(lr)\n",
    "        elif optimizer == \"gd\":\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr)\n",
    "        elif optimizer == \"padagrad\":\n",
    "            train_step = tf.train.ProximalAdagradOptimizer(lr)  # .minimize(self.loss)\n",
    "        elif optimizer == \"pgd\":\n",
    "            train_step = tf.train.ProximalGradientDescentOptimizer(lr)\n",
    "        elif optimizer == \"rmsprop\":\n",
    "            train_step = tf.train.RMSPropOptimizer(lr)\n",
    "        else:\n",
    "            train_step = tf.train.GradientDescentOptimizer(lr)\n",
    "        return train_step\n",
    "\n",
    "    def _build_train_opt(self):\n",
    "        \"\"\"Construct gradient descent based optimization step\n",
    "        In this step, we provide gradient clipping option. Sometimes we what to clip the gradients\n",
    "        when their absolute values are too large to avoid gradient explosion.\n",
    "        Returns:\n",
    "            obj: An operation that applies the specified optimization step.\n",
    "        \"\"\"\n",
    "        train_step = self._train_opt()\n",
    "        gradients, variables = zip(*train_step.compute_gradients(self.loss))\n",
    "        if self.hparams.is_clip_norm:\n",
    "            gradients = [\n",
    "                None\n",
    "                if gradient is None\n",
    "                else tf.clip_by_norm(gradient, self.hparams.max_grad_norm)\n",
    "                for gradient in gradients\n",
    "            ]\n",
    "        return train_step.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    def _active_layer(self, logit, activation, layer_idx=-1):\n",
    "        \"\"\"Transform the input value with an activation. May use dropout.\n",
    "        \n",
    "        Args:\n",
    "            logit (obj): Input value.\n",
    "            activation (str): A string indicating the type of activation function.\n",
    "            layer_idx (int): Index of current layer. Used to retrieve corresponding parameters\n",
    "        \n",
    "        Returns:\n",
    "            obj: A tensor after applying activation function on logit.\n",
    "        \"\"\"\n",
    "        if layer_idx >= 0 and self.hparams.user_dropout:\n",
    "            logit = self._dropout(logit, self.layer_keeps[layer_idx])\n",
    "        return self._activate(logit, activation)\n",
    "\n",
    "    def _activate(self, logit, activation):\n",
    "        if activation == \"sigmoid\":\n",
    "            return tf.nn.sigmoid(logit)\n",
    "        elif activation == \"softmax\":\n",
    "            return tf.nn.softmax(logit)\n",
    "        elif activation == \"relu\":\n",
    "            return tf.nn.relu(logit)\n",
    "        elif activation == \"tanh\":\n",
    "            return tf.nn.tanh(logit)\n",
    "        elif activation == \"elu\":\n",
    "            return tf.nn.elu(logit)\n",
    "        elif activation == \"identity\":\n",
    "            return tf.identity(logit)\n",
    "        else:\n",
    "            raise ValueError(\"this activations not defined {0}\".format(activation))\n",
    "\n",
    "    def _dropout(self, logit, keep_prob):\n",
    "        \"\"\"Apply drops upon the input value.\n",
    "        Args:\n",
    "            logit (obj): The input value.\n",
    "            keep_prob (float): The probability of keeping each element.\n",
    "        Returns:\n",
    "            obj: A tensor of the same shape of logit.\n",
    "        \"\"\"\n",
    "        return tf.nn.dropout(x=logit, keep_prob=keep_prob)\n",
    "\n",
    "    def train(self, sess, feed_dict):\n",
    "        \"\"\"Go through the optimization step once with training data in feed_dict.\n",
    "        Args:\n",
    "            sess (obj): The model session object.\n",
    "            feed_dict (dict): Feed values to train the model. This is a dictionary that maps graph elements to values.\n",
    "        Returns:\n",
    "            list: A list of values, including update operation, total loss, data loss, and merged summary.\n",
    "        \"\"\"\n",
    "        feed_dict[self.layer_keeps] = self.keep_prob_train\n",
    "        feed_dict[self.is_train_stage] = True\n",
    "        return sess.run(\n",
    "            [self.update, self.loss, self.data_loss, self.merged], feed_dict=feed_dict\n",
    "        )\n",
    "\n",
    "    def eval(self, sess, feed_dict):\n",
    "        \"\"\"Evaluate the data in feed_dict with current model.\n",
    "        Args:\n",
    "            sess (obj): The model session object.\n",
    "            feed_dict (dict): Feed values for evaluation. This is a dictionary that maps graph elements to values.\n",
    "        Returns:\n",
    "            list: A list of evaluated results, including total loss value, data loss value,\n",
    "                predicted scores, and ground-truth labels.\n",
    "        \"\"\"\n",
    "        feed_dict[self.layer_keeps] = self.keep_prob_test\n",
    "        feed_dict[self.is_train_stage] = False\n",
    "        return sess.run(\n",
    "            [self.loss, self.data_loss, self.pred, self.iterator.labels],\n",
    "            feed_dict=feed_dict,\n",
    "        )\n",
    "\n",
    "    def infer(self, sess, feed_dict):\n",
    "        \"\"\"Given feature data (in feed_dict), get predicted scores with current model.\n",
    "        Args:\n",
    "            sess (obj): The model session object.\n",
    "            feed_dict (dict): Instances to predict. This is a dictionary that maps graph elements to values.\n",
    "        Returns:\n",
    "            list: Predicted scores for the given instances.\n",
    "        \"\"\"\n",
    "        feed_dict[self.layer_keeps] = self.keep_prob_test\n",
    "        feed_dict[self.is_train_stage] = False\n",
    "        return sess.run([self.pred], feed_dict=feed_dict)\n",
    "\n",
    "    def load_model(self, model_path=None):\n",
    "        \"\"\"Load an existing model.\n",
    "        Args:\n",
    "            model_path: model path.\n",
    "        Raises:\n",
    "            IOError: if the restore operation failed.\n",
    "        \"\"\"\n",
    "        act_path = self.hparams.load_saved_model\n",
    "        if model_path is not None:\n",
    "            act_path = model_path\n",
    "\n",
    "        try:\n",
    "            self.saver.restore(self.sess, act_path)\n",
    "        except:\n",
    "            raise IOError(\"Failed to find any matching files for {0}\".format(act_path))\n",
    "\n",
    "    def fit(self, train_file, valid_file, test_file=None):\n",
    "        \"\"\"Fit the model with train_file. Evaluate the model on valid_file per epoch to observe the training status.\n",
    "        If test_file is not None, evaluate it too.\n",
    "        \n",
    "        Args:\n",
    "            train_file (str): training data set.\n",
    "            valid_file (str): validation set.\n",
    "            test_file (str): test set.\n",
    "        Returns:\n",
    "            obj: An instance of self.\n",
    "        \"\"\"\n",
    "        if self.hparams.write_tfevents:\n",
    "            self.writer = tf.summary.FileWriter(\n",
    "                self.hparams.SUMMARIES_DIR, self.sess.graph\n",
    "            )\n",
    "\n",
    "        train_sess = self.sess\n",
    "        for epoch in range(1, self.hparams.epochs + 1):\n",
    "            step = 0\n",
    "            self.hparams.current_epoch = epoch\n",
    "\n",
    "            epoch_loss = 0\n",
    "            train_start = time.time()\n",
    "            for batch_data_input in self.iterator.load_data_from_file(train_file):\n",
    "                step_result = self.train(train_sess, batch_data_input)\n",
    "                (_, step_loss, step_data_loss, summary) = step_result\n",
    "                if self.hparams.write_tfevents:\n",
    "                    self.writer.add_summary(summary, step)\n",
    "                epoch_loss += step_loss\n",
    "                step += 1\n",
    "                if step % self.hparams.show_step == 0:\n",
    "                    print(\n",
    "                        \"step {0:d} , total_loss: {1:.4f}, data_loss: {2:.4f}\".format(\n",
    "                            step, step_loss, step_data_loss\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            train_end = time.time()\n",
    "            train_time = train_end - train_start\n",
    "\n",
    "            if self.hparams.save_model:\n",
    "                if epoch % self.hparams.save_epoch == 0:\n",
    "                    checkpoint_path = self.saver.save(\n",
    "                        sess=train_sess,\n",
    "                        save_path=self.hparams.MODEL_DIR + \"epoch_\" + str(epoch),\n",
    "                    )\n",
    "\n",
    "            eval_start = time.time()\n",
    "            train_res = self.run_eval(train_file)\n",
    "            eval_res = self.run_eval(valid_file)\n",
    "            train_info = \", \".join(\n",
    "                [\n",
    "                    str(item[0]) + \":\" + str(item[1])\n",
    "                    for item in sorted(train_res.items(), key=lambda x: x[0])\n",
    "                ]\n",
    "            )\n",
    "            eval_info = \", \".join(\n",
    "                [\n",
    "                    str(item[0]) + \":\" + str(item[1])\n",
    "                    for item in sorted(eval_res.items(), key=lambda x: x[0])\n",
    "                ]\n",
    "            )\n",
    "            if test_file is not None:\n",
    "                test_res = self.run_eval(test_file)\n",
    "                test_info = \", \".join(\n",
    "                    [\n",
    "                        str(item[0]) + \":\" + str(item[1])\n",
    "                        for item in sorted(test_res.items(), key=lambda x: x[0])\n",
    "                    ]\n",
    "                )\n",
    "            eval_end = time.time()\n",
    "            eval_time = eval_end - eval_start\n",
    "\n",
    "            if test_file is not None:\n",
    "                print(\n",
    "                    \"at epoch {0:d}\".format(epoch)\n",
    "                    + \" train info: \"\n",
    "                    + train_info\n",
    "                    + \" eval info: \"\n",
    "                    + eval_info\n",
    "                    + \" test info: \"\n",
    "                    + test_info\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"at epoch {0:d}\".format(epoch)\n",
    "                    + \" train info: \"\n",
    "                    + train_info\n",
    "                    + \" eval info: \"\n",
    "                    + eval_info\n",
    "                )\n",
    "            print(\n",
    "                \"at epoch {0:d} , train time: {1:.1f} eval time: {2:.1f}\".format(\n",
    "                    epoch, train_time, eval_time\n",
    "                )\n",
    "            )\n",
    "\n",
    "        if self.hparams.write_tfevents:\n",
    "            self.writer.close()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def run_eval(self, filename):\n",
    "        \"\"\"Evaluate the given file and returns some evaluation metrics.\n",
    "        \n",
    "        Args:\n",
    "            filename (str): A file name that will be evaluated.\n",
    "        Returns:\n",
    "            dict: A dictionary contains evaluation metrics.\n",
    "        \"\"\"\n",
    "        load_sess = self.sess\n",
    "        preds = []\n",
    "        labels = []\n",
    "        for batch_data_input in self.iterator.load_data_from_file(filename):\n",
    "            _, _, step_pred, step_labels = self.eval(load_sess, batch_data_input)\n",
    "            preds.extend(np.reshape(step_pred, -1))\n",
    "            labels.extend(np.reshape(step_labels, -1))\n",
    "        res = cal_metric(labels, preds, self.hparams.metrics)\n",
    "        return res\n",
    "\n",
    "    def predict(self, infile_name, outfile_name):\n",
    "        \"\"\"Make predictions on the given data, and output predicted scores to a file.\n",
    "        \n",
    "        Args:\n",
    "            infile_name (str): Input file name.\n",
    "            outfile_name (str): Output file name.\n",
    "        Returns:\n",
    "            obj: An instance of self.\n",
    "        \"\"\"\n",
    "        load_sess = self.sess\n",
    "        preds = []\n",
    "        with tf.gfile.GFile(outfile_name, \"w\") as wt:\n",
    "            for batch_data_input in self.iterator.load_data_from_file(infile_name):\n",
    "                step_pred = self.infer(load_sess, batch_data_input)\n",
    "                step_pred = np.reshape(step_pred, -1)\n",
    "                preds.extend(step_pred)\n",
    "                wt.write(\"\\n\".join(map(str, step_pred)))\n",
    "                # line break after each batch.\n",
    "                wt.write(\"\\n\")\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "__all__ = [\"XDeepFMModel\"]\n",
    "\n",
    "\n",
    "class XDeepFMModel(BaseModel):\n",
    "    def _build_graph(self):\n",
    "        \"\"\"The main function to create xdeepfm's logic.\n",
    "        \n",
    "        Returns:\n",
    "            obj:the prediction score make by the model.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        self.keep_prob_train = 1 - np.array(hparams.dropout)\n",
    "        self.keep_prob_test = np.ones_like(hparams.dropout)\n",
    "\n",
    "        with tf.variable_scope(\"XDeepFM\") as scope:\n",
    "            with tf.variable_scope(\"embedding\", initializer=self.initializer) as escope:\n",
    "                self.embedding = tf.get_variable(\n",
    "                    name=\"embedding_layer\",\n",
    "                    shape=[hparams.FEATURE_COUNT, hparams.dim],\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                self.embed_params.append(self.embedding)\n",
    "                embed_out, embed_layer_size = self._build_embedding()\n",
    "\n",
    "            logit = 0\n",
    "\n",
    "            if hparams.use_Linear_part:\n",
    "                print(\"Add linear part.\")\n",
    "                logit = logit + self._build_linear()\n",
    "\n",
    "            if hparams.use_FM_part:\n",
    "                print(\"Add FM part.\")\n",
    "                logit = logit + self._build_fm()\n",
    "\n",
    "            if hparams.use_CIN_part:\n",
    "                print(\"Add CIN part.\")\n",
    "                if hparams.fast_CIN_d <= 0:\n",
    "                    logit = logit + self._build_CIN(\n",
    "                        embed_out, res=True, direct=False, bias=False, is_masked=True\n",
    "                    )\n",
    "                else:\n",
    "                    logit = logit + self._build_fast_CIN(\n",
    "                        embed_out, res=True, direct=False, bias=False\n",
    "                    )\n",
    "\n",
    "            if hparams.use_DNN_part:\n",
    "                print(\"Add DNN part.\")\n",
    "                logit = logit + self._build_dnn(embed_out, embed_layer_size)\n",
    "\n",
    "            return logit\n",
    "\n",
    "    def _build_embedding(self):\n",
    "        \"\"\"The field embedding layer. MLP requires fixed-length vectors as input.\n",
    "        This function makes sum pooling of feature embeddings for each field.\n",
    "        \n",
    "        Returns:\n",
    "            embedding:  the result of field embedding layer, with size of #_fields * #_dim\n",
    "            embedding_size: #_fields * #_dim\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        fm_sparse_index = tf.SparseTensor(\n",
    "            self.iterator.dnn_feat_indices,\n",
    "            self.iterator.dnn_feat_values,\n",
    "            self.iterator.dnn_feat_shape,\n",
    "        )\n",
    "        fm_sparse_weight = tf.SparseTensor(\n",
    "            self.iterator.dnn_feat_indices,\n",
    "            self.iterator.dnn_feat_weights,\n",
    "            self.iterator.dnn_feat_shape,\n",
    "        )\n",
    "        w_fm_nn_input_orgin = tf.nn.embedding_lookup_sparse(\n",
    "            self.embedding, fm_sparse_index, fm_sparse_weight, combiner=\"sum\"\n",
    "        )\n",
    "        embedding = tf.reshape(\n",
    "            w_fm_nn_input_orgin, [-1, hparams.dim * hparams.FIELD_COUNT]\n",
    "        )\n",
    "        embedding_size = hparams.FIELD_COUNT * hparams.dim\n",
    "        return embedding, embedding_size\n",
    "\n",
    "    def _build_linear(self):\n",
    "        \"\"\"Construct the linear part for the model.\n",
    "        This is a linear regression.\n",
    "        \n",
    "        Returns:\n",
    "            obj: prediction score made by linear regression.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"linear_part\", initializer=self.initializer) as scope:\n",
    "            w = tf.get_variable(\n",
    "                name=\"w\", shape=[self.hparams.FEATURE_COUNT, 1], dtype=tf.float32\n",
    "            )\n",
    "            b = tf.get_variable(\n",
    "                name=\"b\",\n",
    "                shape=[1],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer(),\n",
    "            )\n",
    "            x = tf.SparseTensor(\n",
    "                self.iterator.fm_feat_indices,\n",
    "                self.iterator.fm_feat_values,\n",
    "                self.iterator.fm_feat_shape,\n",
    "            )\n",
    "            linear_output = tf.add(tf.sparse_tensor_dense_matmul(x, w), b)\n",
    "            self.layer_params.append(w)\n",
    "            self.layer_params.append(b)\n",
    "            tf.summary.histogram(\"linear_part/w\", w)\n",
    "            tf.summary.histogram(\"linear_part/b\", b)\n",
    "            return linear_output\n",
    "\n",
    "    def _build_fm(self):\n",
    "        \"\"\"Construct the factorization machine part for the model.\n",
    "        This is a traditional 2-order FM module.\n",
    "        \n",
    "        Returns:\n",
    "            obj: prediction score made by factorization machine.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"fm_part\") as scope:\n",
    "            x = tf.SparseTensor(\n",
    "                self.iterator.fm_feat_indices,\n",
    "                self.iterator.fm_feat_values,\n",
    "                self.iterator.fm_feat_shape,\n",
    "            )\n",
    "            xx = tf.SparseTensor(\n",
    "                self.iterator.fm_feat_indices,\n",
    "                tf.pow(self.iterator.fm_feat_values, 2),\n",
    "                self.iterator.fm_feat_shape,\n",
    "            )\n",
    "            fm_output = 0.5 * tf.reduce_sum(\n",
    "                tf.pow(tf.sparse_tensor_dense_matmul(x, self.embedding), 2)\n",
    "                - tf.sparse_tensor_dense_matmul(xx, tf.pow(self.embedding, 2)),\n",
    "                1,\n",
    "                keep_dims=True,\n",
    "            )\n",
    "            return fm_output\n",
    "\n",
    "    def _build_CIN(\n",
    "        self, nn_input, res=False, direct=False, bias=False, is_masked=False\n",
    "    ):\n",
    "        \"\"\"Construct the compressed interaction network.\n",
    "        This component provides explicit and vector-wise higher-order feature interactions.\n",
    "        \n",
    "        Args:\n",
    "            nn_input (obj): The output of field-embedding layer. This is the input for CIN.\n",
    "            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\n",
    "            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\n",
    "                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\n",
    "            bias (bool): Whether to add bias term when calculating the feature maps.\n",
    "            is_masked (bool): Controls whether to remove self-interaction in the first layer of CIN.\n",
    "        \n",
    "        Returns:\n",
    "            obj: prediction score made by CIN.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        hidden_nn_layers = []\n",
    "        field_nums = []\n",
    "        final_len = 0\n",
    "        field_num = hparams.FIELD_COUNT\n",
    "        nn_input = tf.reshape(nn_input, shape=[-1, int(field_num), hparams.dim])\n",
    "        field_nums.append(int(field_num))\n",
    "        hidden_nn_layers.append(nn_input)\n",
    "        final_result = []\n",
    "        split_tensor0 = tf.split(hidden_nn_layers[0], hparams.dim * [1], 2)\n",
    "        with tf.variable_scope(\"exfm_part\", initializer=self.initializer) as scope:\n",
    "            for idx, layer_size in enumerate(hparams.cross_layer_sizes):\n",
    "                split_tensor = tf.split(hidden_nn_layers[-1], hparams.dim * [1], 2)\n",
    "                dot_result_m = tf.matmul(\n",
    "                    split_tensor0, split_tensor, transpose_b=True\n",
    "                )  # shape :  (Dim, Batch, FieldNum, HiddenNum), a.k.a (D,B,F,H)\n",
    "                dot_result_o = tf.reshape(\n",
    "                    dot_result_m,\n",
    "                    shape=[hparams.dim, -1, field_nums[0] * field_nums[-1]],\n",
    "                )  # shape: (D,B,FH)\n",
    "                dot_result = tf.transpose(dot_result_o, perm=[1, 0, 2])  # (B,D,FH)\n",
    "\n",
    "                filters = tf.get_variable(\n",
    "                    name=\"f_\" + str(idx),\n",
    "                    shape=[1, field_nums[-1] * field_nums[0], layer_size],\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "\n",
    "                if is_masked and idx == 0:\n",
    "                    ones = tf.ones([field_nums[0], field_nums[0]], dtype=tf.float32)\n",
    "                    mask_matrix = tf.matrix_band_part(ones, 0, -1) - tf.diag(\n",
    "                        tf.ones(field_nums[0])\n",
    "                    )\n",
    "                    mask_matrix = tf.reshape(\n",
    "                        mask_matrix, shape=[1, field_nums[0] * field_nums[0]]\n",
    "                    )\n",
    "\n",
    "                    dot_result = tf.multiply(dot_result, mask_matrix) * 2\n",
    "                    self.dot_result = dot_result\n",
    "\n",
    "                curr_out = tf.nn.conv1d(\n",
    "                    dot_result, filters=filters, stride=1, padding=\"VALID\"\n",
    "                )  # shape : (B,D,H`)\n",
    "\n",
    "                if bias:\n",
    "                    b = tf.get_variable(\n",
    "                        name=\"f_b\" + str(idx),\n",
    "                        shape=[layer_size],\n",
    "                        dtype=tf.float32,\n",
    "                        initializer=tf.zeros_initializer(),\n",
    "                    )\n",
    "                    curr_out = tf.nn.bias_add(curr_out, b)\n",
    "                    self.cross_params.append(b)\n",
    "\n",
    "                if hparams.enable_BN is True:\n",
    "                    curr_out = tf.layers.batch_normalization(\n",
    "                        curr_out,\n",
    "                        momentum=0.95,\n",
    "                        epsilon=0.0001,\n",
    "                        training=self.is_train_stage,\n",
    "                    )\n",
    "\n",
    "                curr_out = self._activate(curr_out, hparams.cross_activation)\n",
    "\n",
    "                curr_out = tf.transpose(curr_out, perm=[0, 2, 1])  # shape : (B,H,D)\n",
    "\n",
    "                if direct:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = curr_out\n",
    "                    final_len += layer_size\n",
    "                    field_nums.append(int(layer_size))\n",
    "\n",
    "                else:\n",
    "                    if idx != len(hparams.cross_layer_sizes) - 1:\n",
    "                        next_hidden, direct_connect = tf.split(\n",
    "                            curr_out, 2 * [int(layer_size / 2)], 1\n",
    "                        )\n",
    "                        final_len += int(layer_size / 2)\n",
    "                    else:\n",
    "                        direct_connect = curr_out\n",
    "                        next_hidden = 0\n",
    "                        final_len += layer_size\n",
    "                    field_nums.append(int(layer_size / 2))\n",
    "\n",
    "                final_result.append(direct_connect)\n",
    "                hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "                self.cross_params.append(filters)\n",
    "\n",
    "            result = tf.concat(final_result, axis=1)\n",
    "            result = tf.reduce_sum(result, -1)  # shape : (B,H)\n",
    "\n",
    "            if res:\n",
    "                base_score = tf.reduce_sum(result, 1, keepdims=True)  # (B,1)\n",
    "            else:\n",
    "                base_score = 0\n",
    "\n",
    "            w_nn_output = tf.get_variable(\n",
    "                name=\"w_nn_output\", shape=[final_len, 1], dtype=tf.float32\n",
    "            )\n",
    "            b_nn_output = tf.get_variable(\n",
    "                name=\"b_nn_output\",\n",
    "                shape=[1],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer(),\n",
    "            )\n",
    "            self.layer_params.append(w_nn_output)\n",
    "            self.layer_params.append(b_nn_output)\n",
    "            exFM_out = base_score + tf.nn.xw_plus_b(result, w_nn_output, b_nn_output)\n",
    "            return exFM_out\n",
    "\n",
    "    def _build_fast_CIN(self, nn_input, res=False, direct=False, bias=False):\n",
    "        \"\"\"Construct the compressed interaction network with reduced parameters.\n",
    "        This component provides explicit and vector-wise higher-order feature interactions.\n",
    "        Parameters from the filters are reduced via a matrix decomposition method.\n",
    "        Fast CIN is more space and time efficient than CIN.\n",
    "        \n",
    "        Args:\n",
    "            nn_input (obj): The output of field-embedding layer. This is the input for CIN.\n",
    "            res (bool): Whether use residual structure to fuse the results from each layer of CIN.\n",
    "            direct (bool): If true, then all hidden units are connected to both next layer and output layer;\n",
    "                    otherwise, half of hidden units are connected to next layer and the other half will be connected to output layer.\n",
    "            bias (bool): Whether to add bias term when calculating the feature maps.\n",
    "        Returns:\n",
    "            obj: prediction score made by fast CIN.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        hidden_nn_layers = []\n",
    "        field_nums = []\n",
    "        final_len = 0\n",
    "        field_num = hparams.FIELD_COUNT\n",
    "        fast_CIN_d = hparams.fast_CIN_d\n",
    "        nn_input = tf.reshape(\n",
    "            nn_input, shape=[-1, int(field_num), hparams.dim]\n",
    "        )  # (B,F,D)\n",
    "        nn_input = tf.transpose(nn_input, perm=[0, 2, 1])  # (B,D,F)\n",
    "        field_nums.append(int(field_num))\n",
    "        hidden_nn_layers.append(nn_input)\n",
    "        final_result = []\n",
    "        with tf.variable_scope(\"exfm_part\", initializer=self.initializer) as scope:\n",
    "            for idx, layer_size in enumerate(hparams.cross_layer_sizes):\n",
    "                if idx == 0:\n",
    "                    fast_w = tf.get_variable(\n",
    "                        \"fast_CIN_w_\" + str(idx),\n",
    "                        shape=[1, field_nums[0], fast_CIN_d * layer_size],\n",
    "                        dtype=tf.float32,\n",
    "                    )\n",
    "\n",
    "                    self.cross_params.append(fast_w)\n",
    "                    dot_result_1 = tf.nn.conv1d(\n",
    "                        nn_input, filters=fast_w, stride=1, padding=\"VALID\"\n",
    "                    )  # shape: (B,D,d*H)\n",
    "                    dot_result_2 = tf.nn.conv1d(\n",
    "                        tf.pow(nn_input, 2),\n",
    "                        filters=tf.pow(fast_w, 2),\n",
    "                        stride=1,\n",
    "                        padding=\"VALID\",\n",
    "                    )  # shape: ((B,D,d*H)\n",
    "                    dot_result = tf.reshape(\n",
    "                        0.5 * (dot_result_1 - dot_result_2),\n",
    "                        shape=[-1, hparams.dim, layer_size, fast_CIN_d],\n",
    "                    )\n",
    "                    curr_out = tf.reduce_sum(\n",
    "                        dot_result, 3, keepdims=False\n",
    "                    )  # shape: ((B,D,H)\n",
    "                else:\n",
    "                    fast_w = tf.get_variable(\n",
    "                        \"fast_CIN_w_\" + str(idx),\n",
    "                        shape=[1, field_nums[0], fast_CIN_d * layer_size],\n",
    "                        dtype=tf.float32,\n",
    "                    )\n",
    "                    fast_v = tf.get_variable(\n",
    "                        \"fast_CIN_v_\" + str(idx),\n",
    "                        shape=[1, field_nums[-1], fast_CIN_d * layer_size],\n",
    "                        dtype=tf.float32,\n",
    "                    )\n",
    "\n",
    "                    self.cross_params.append(fast_w)\n",
    "                    self.cross_params.append(fast_v)\n",
    "\n",
    "                    dot_result_1 = tf.nn.conv1d(\n",
    "                        nn_input, filters=fast_w, stride=1, padding=\"VALID\"\n",
    "                    )  # shape: ((B,D,d*H)\n",
    "                    dot_result_2 = tf.nn.conv1d(\n",
    "                        hidden_nn_layers[-1], filters=fast_v, stride=1, padding=\"VALID\"\n",
    "                    )  # shape: ((B,D,d*H)\n",
    "                    dot_result = tf.reshape(\n",
    "                        tf.multiply(dot_result_1, dot_result_2),\n",
    "                        shape=[-1, hparams.dim, layer_size, fast_CIN_d],\n",
    "                    )\n",
    "                    curr_out = tf.reduce_sum(\n",
    "                        dot_result, 3, keepdims=False\n",
    "                    )  # shape: ((B,D,H)\n",
    "\n",
    "                if bias:\n",
    "                    b = tf.get_variable(\n",
    "                        name=\"f_b\" + str(idx),\n",
    "                        shape=[1, 1, layer_size],\n",
    "                        dtype=tf.float32,\n",
    "                        initializer=tf.zeros_initializer(),\n",
    "                    )\n",
    "                    curr_out = tf.nn.bias_add(curr_out, b)\n",
    "                    self.cross_params.append(b)\n",
    "\n",
    "                if hparams.enable_BN is True:\n",
    "                    curr_out = tf.layers.batch_normalization(\n",
    "                        curr_out,\n",
    "                        momentum=0.95,\n",
    "                        epsilon=0.0001,\n",
    "                        training=self.is_train_stage,\n",
    "                    )\n",
    "\n",
    "                curr_out = self._activate(curr_out, hparams.cross_activation)\n",
    "\n",
    "                if direct:\n",
    "                    direct_connect = curr_out\n",
    "                    next_hidden = curr_out\n",
    "                    final_len += layer_size\n",
    "                    field_nums.append(int(layer_size))\n",
    "\n",
    "                else:\n",
    "                    if idx != len(hparams.cross_layer_sizes) - 1:\n",
    "                        next_hidden, direct_connect = tf.split(\n",
    "                            curr_out, 2 * [int(layer_size / 2)], 2\n",
    "                        )\n",
    "                        final_len += int(layer_size / 2)\n",
    "                        field_nums.append(int(layer_size / 2))\n",
    "                    else:\n",
    "                        direct_connect = curr_out\n",
    "                        next_hidden = 0\n",
    "                        final_len += layer_size\n",
    "                        field_nums.append(int(layer_size))\n",
    "\n",
    "                final_result.append(direct_connect)\n",
    "                hidden_nn_layers.append(next_hidden)\n",
    "\n",
    "            result = tf.concat(final_result, axis=2)\n",
    "            result = tf.reduce_sum(result, 1, keepdims=False)  # (B,H)\n",
    "\n",
    "            if res:\n",
    "                base_score = tf.reduce_sum(result, 1, keepdims=True)  # (B,1)\n",
    "            else:\n",
    "                base_score = 0\n",
    "\n",
    "            w_nn_output = tf.get_variable(\n",
    "                name=\"w_nn_output\", shape=[final_len, 1], dtype=tf.float32\n",
    "            )\n",
    "            b_nn_output = tf.get_variable(\n",
    "                name=\"b_nn_output\",\n",
    "                shape=[1],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer(),\n",
    "            )\n",
    "            self.layer_params.append(w_nn_output)\n",
    "            self.layer_params.append(b_nn_output)\n",
    "            exFM_out = tf.nn.xw_plus_b(result, w_nn_output, b_nn_output) + base_score\n",
    "\n",
    "        return exFM_out\n",
    "\n",
    "    def _build_dnn(self, embed_out, embed_layer_size):\n",
    "        \"\"\"Construct the MLP part for the model.\n",
    "        This components provides implicit higher-order feature interactions.\n",
    "        \n",
    "        Args:\n",
    "            embed_out (obj): The output of field-embedding layer. This is the input for DNN.\n",
    "            embed_layer_size (obj): shape of the embed_out\n",
    "        Returns:\n",
    "            obj: prediction score made by fast CIN.\n",
    "        \"\"\"\n",
    "        hparams = self.hparams\n",
    "        w_fm_nn_input = embed_out\n",
    "        last_layer_size = embed_layer_size\n",
    "        layer_idx = 0\n",
    "        hidden_nn_layers = []\n",
    "        hidden_nn_layers.append(w_fm_nn_input)\n",
    "        with tf.variable_scope(\"nn_part\", initializer=self.initializer) as scope:\n",
    "            for idx, layer_size in enumerate(hparams.layer_sizes):\n",
    "                curr_w_nn_layer = tf.get_variable(\n",
    "                    name=\"w_nn_layer\" + str(layer_idx),\n",
    "                    shape=[last_layer_size, layer_size],\n",
    "                    dtype=tf.float32,\n",
    "                )\n",
    "                curr_b_nn_layer = tf.get_variable(\n",
    "                    name=\"b_nn_layer\" + str(layer_idx),\n",
    "                    shape=[layer_size],\n",
    "                    dtype=tf.float32,\n",
    "                    initializer=tf.zeros_initializer(),\n",
    "                )\n",
    "                tf.summary.histogram(\n",
    "                    \"nn_part/\" + \"w_nn_layer\" + str(layer_idx), curr_w_nn_layer\n",
    "                )\n",
    "                tf.summary.histogram(\n",
    "                    \"nn_part/\" + \"b_nn_layer\" + str(layer_idx), curr_b_nn_layer\n",
    "                )\n",
    "                curr_hidden_nn_layer = tf.nn.xw_plus_b(\n",
    "                    hidden_nn_layers[layer_idx], curr_w_nn_layer, curr_b_nn_layer\n",
    "                )\n",
    "                scope = \"nn_part\" + str(idx)\n",
    "                activation = hparams.activation[idx]\n",
    "\n",
    "                if hparams.enable_BN is True:\n",
    "                    curr_hidden_nn_layer = tf.layers.batch_normalization(\n",
    "                        curr_hidden_nn_layer,\n",
    "                        momentum=0.95,\n",
    "                        epsilon=0.0001,\n",
    "                        training=self.is_train_stage,\n",
    "                    )\n",
    "\n",
    "                curr_hidden_nn_layer = self._active_layer(\n",
    "                    logit=curr_hidden_nn_layer, activation=activation, layer_idx=idx\n",
    "                )\n",
    "                hidden_nn_layers.append(curr_hidden_nn_layer)\n",
    "                layer_idx += 1\n",
    "                last_layer_size = layer_size\n",
    "                self.layer_params.append(curr_w_nn_layer)\n",
    "                self.layer_params.append(curr_b_nn_layer)\n",
    "\n",
    "            w_nn_output = tf.get_variable(\n",
    "                name=\"w_nn_output\", shape=[last_layer_size, 1], dtype=tf.float32\n",
    "            )\n",
    "            b_nn_output = tf.get_variable(\n",
    "                name=\"b_nn_output\",\n",
    "                shape=[1],\n",
    "                dtype=tf.float32,\n",
    "                initializer=tf.zeros_initializer(),\n",
    "            )\n",
    "            tf.summary.histogram(\n",
    "                \"nn_part/\" + \"w_nn_output\" + str(layer_idx), w_nn_output\n",
    "            )\n",
    "            tf.summary.histogram(\n",
    "                \"nn_part/\" + \"b_nn_output\" + str(layer_idx), b_nn_output\n",
    "            )\n",
    "            self.layer_params.append(w_nn_output)\n",
    "            self.layer_params.append(b_nn_output)\n",
    "            nn_output = tf.nn.xw_plus_b(hidden_nn_layers[-1], w_nn_output, b_nn_output)\n",
    "            return nn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 100\n",
    "EPOCHS_FOR_CRITEO_RUN = 8\n",
    "BATCH_SIZE_CRITEO = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "def flat_config(config):\n",
    "    \"\"\"Flat config loaded from a yaml file to a flat dict.\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration loaded from a yaml file.\n",
    "    Returns:\n",
    "        dict: Configuration dictionary.\n",
    "    \"\"\"\n",
    "    f_config = {}\n",
    "    category = config.keys()\n",
    "    for cate in category:\n",
    "        for key, val in config[cate].items():\n",
    "            f_config[key] = val\n",
    "    return f_config\n",
    "\n",
    "\n",
    "def check_type(config):\n",
    "    \"\"\"Check that the config parameters are the correct type\n",
    "    \n",
    "    Args:\n",
    "        config (dict): Configuration dictionary.\n",
    "    Raises:\n",
    "        TypeError: If the parameters are not the correct type.\n",
    "    \"\"\"\n",
    "\n",
    "    int_parameters = [\n",
    "        \"word_size\",\n",
    "        \"entity_size\",\n",
    "        \"doc_size\",\n",
    "        \"FEATURE_COUNT\",\n",
    "        \"FIELD_COUNT\",\n",
    "        \"dim\",\n",
    "        \"epochs\",\n",
    "        \"batch_size\",\n",
    "        \"show_step\",\n",
    "        \"save_epoch\",\n",
    "        \"PAIR_NUM\",\n",
    "        \"DNN_FIELD_NUM\",\n",
    "        \"attention_layer_sizes\",\n",
    "        \"n_user\",\n",
    "        \"n_item\",\n",
    "        \"n_user_attr\",\n",
    "        \"n_item_attr\",\n",
    "    ]\n",
    "    for param in int_parameters:\n",
    "        if param in config and not isinstance(config[param], int):\n",
    "            raise TypeError(\"Parameters {0} must be int\".format(param))\n",
    "\n",
    "    float_parameters = [\n",
    "        \"init_value\",\n",
    "        \"learning_rate\",\n",
    "        \"embed_l2\",\n",
    "        \"embed_l1\",\n",
    "        \"layer_l2\",\n",
    "        \"layer_l1\",\n",
    "        \"mu\",\n",
    "    ]\n",
    "    for param in float_parameters:\n",
    "        if param in config and not isinstance(config[param], float):\n",
    "            raise TypeError(\"Parameters {0} must be float\".format(param))\n",
    "\n",
    "    str_parameters = [\n",
    "        \"train_file\",\n",
    "        \"eval_file\",\n",
    "        \"test_file\",\n",
    "        \"infer_file\",\n",
    "        \"method\",\n",
    "        \"load_model_name\",\n",
    "        \"infer_model_name\",\n",
    "        \"loss\",\n",
    "        \"optimizer\",\n",
    "        \"init_method\",\n",
    "        \"attention_activation\",\n",
    "    ]\n",
    "    for param in str_parameters:\n",
    "        if param in config and not isinstance(config[param], str):\n",
    "            raise TypeError(\"Parameters {0} must be str\".format(param))\n",
    "\n",
    "    list_parameters = [\"layer_sizes\", \"activation\", \"dropout\"]\n",
    "    for param in list_parameters:\n",
    "        if param in config and not isinstance(config[param], list):\n",
    "            raise TypeError(\"Parameters {0} must be list\".format(param))\n",
    "\n",
    "\n",
    "def check_nn_config(f_config):\n",
    "    \"\"\"Check neural networks configuration.\n",
    "    \n",
    "    Args:\n",
    "        f_config (dict): Neural network configuration.\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If the parameters are not correct.\n",
    "    \"\"\"\n",
    "    if f_config[\"model_type\"] in [\"fm\", \"FM\"]:\n",
    "        required_parameters = [\"FEATURE_COUNT\", \"dim\", \"loss\", \"data_format\", \"method\"]\n",
    "    elif f_config[\"model_type\"] in [\"lr\", \"LR\"]:\n",
    "        required_parameters = [\"FEATURE_COUNT\", \"loss\", \"data_format\", \"method\"]\n",
    "    elif f_config[\"model_type\"] in [\"dkn\", \"DKN\"]:\n",
    "        required_parameters = [\n",
    "            \"doc_size\",\n",
    "            \"wordEmb_file\",\n",
    "            \"entityEmb_file\",\n",
    "            \"word_size\",\n",
    "            \"entity_size\",\n",
    "            \"data_format\",\n",
    "            \"dim\",\n",
    "            \"layer_sizes\",\n",
    "            \"activation\",\n",
    "            \"attention_activation\",\n",
    "            \"attention_activation\",\n",
    "            \"attention_dropout\",\n",
    "            \"loss\",\n",
    "            \"data_format\",\n",
    "            \"dropout\",\n",
    "            \"method\",\n",
    "            \"num_filters\",\n",
    "            \"filter_sizes\",\n",
    "        ]\n",
    "    elif f_config[\"model_type\"] in [\"exDeepFM\", \"xDeepFM\"]:\n",
    "        required_parameters = [\n",
    "            \"FIELD_COUNT\",\n",
    "            \"FEATURE_COUNT\",\n",
    "            \"method\",\n",
    "            \"dim\",\n",
    "            \"layer_sizes\",\n",
    "            \"cross_layer_sizes\",\n",
    "            \"activation\",\n",
    "            \"loss\",\n",
    "            \"data_format\",\n",
    "            \"dropout\",\n",
    "        ]\n",
    "    else:\n",
    "        required_parameters = [\n",
    "            \"FIELD_COUNT\",\n",
    "            \"FEATURE_COUNT\",\n",
    "            \"method\",\n",
    "            \"dim\",\n",
    "            \"layer_sizes\",\n",
    "            \"activation\",\n",
    "            \"loss\",\n",
    "            \"data_format\",\n",
    "            \"dropout\",\n",
    "        ]\n",
    "\n",
    "    # check required parameters\n",
    "    for param in required_parameters:\n",
    "        if param not in f_config:\n",
    "            raise ValueError(\"Parameters {0} must be set\".format(param))\n",
    "\n",
    "    if f_config[\"model_type\"] in [\"exDeepFM\", \"xDeepFM\"]:\n",
    "        if f_config[\"data_format\"] != \"ffm\":\n",
    "            raise ValueError(\n",
    "                \"For xDeepFM model, data format must be 'ffm', but your set is {0}\".format(\n",
    "                    f_config[\"data_format\"]\n",
    "                )\n",
    "            )\n",
    "    elif f_config[\"model_type\"] in [\"dkn\", \"DKN\"]:\n",
    "        if f_config[\"data_format\"] != \"dkn\":\n",
    "            raise ValueError(\n",
    "                \"For dkn model, data format must be 'dkn', but your set is {0}\".format(\n",
    "                    f_config[\"data_format\"]\n",
    "                )\n",
    "            )\n",
    "    else:\n",
    "        if f_config[\"data_format\"] not in [\"fm\"]:\n",
    "            raise ValueError(\n",
    "                \"The default data format should be fm, but your set is {0}\".format(\n",
    "                    f_config[\"data_format\"]\n",
    "                )\n",
    "            )\n",
    "    check_type(f_config)\n",
    "\n",
    "\n",
    "def load_yaml(filename):\n",
    "    \"\"\"Load a yaml file.\n",
    "    Args:\n",
    "        filename (str): Filename.\n",
    "    Returns:\n",
    "        dict: Dictionary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, \"r\") as f:\n",
    "            config = yaml.load(f, yaml.SafeLoader)\n",
    "        return config\n",
    "    except FileNotFoundError: # for file not found\n",
    "        raise\n",
    "    except Exception as e: # for other exceptions\n",
    "        raise IOError(\"load {0} error!\".format(filename))\n",
    "def create_hparams(flags):\n",
    "    \"\"\"Create the model hyperparameters.\n",
    "    Args:\n",
    "        flags (dict): Dictionary with the model requirements.\n",
    "    Returns:\n",
    "        obj: Hyperparameter object in TF (tf.contrib.training.HParams).\n",
    "    \"\"\"\n",
    "    return tf.contrib.training.HParams(\n",
    "        # data\n",
    "        kg_file=flags[\"kg_file\"] if \"kg_file\" in flags else None,\n",
    "        user_clicks=flags[\"user_clicks\"] if \"user_clicks\" in flags else None,\n",
    "        FEATURE_COUNT=flags[\"FEATURE_COUNT\"] if \"FEATURE_COUNT\" in flags else None,\n",
    "        FIELD_COUNT=flags[\"FIELD_COUNT\"] if \"FIELD_COUNT\" in flags else None,\n",
    "        data_format=flags[\"data_format\"] if \"data_format\" in flags else None,\n",
    "        PAIR_NUM=flags[\"PAIR_NUM\"] if \"PAIR_NUM\" in flags else None,\n",
    "        DNN_FIELD_NUM=flags[\"DNN_FIELD_NUM\"] if \"DNN_FIELD_NUM\" in flags else None,\n",
    "        n_user=flags[\"n_user\"] if \"n_user\" in flags else None,\n",
    "        n_item=flags[\"n_item\"] if \"n_item\" in flags else None,\n",
    "        n_user_attr=flags[\"n_user_attr\"] if \"n_user_attr\" in flags else None,\n",
    "        n_item_attr=flags[\"n_item_attr\"] if \"n_item_attr\" in flags else None,\n",
    "        iterator_type=flags[\"iterator_type\"] if \"iterator_type\" in flags else None,\n",
    "        SUMMARIES_DIR=flags[\"SUMMARIES_DIR\"] if \"SUMMARIES_DIR\" in flags else None,\n",
    "        MODEL_DIR=flags[\"MODEL_DIR\"] if \"MODEL_DIR\" in flags else None,\n",
    "        # dkn\n",
    "        wordEmb_file=flags[\"wordEmb_file\"] if \"wordEmb_file\" in flags else None,\n",
    "        entityEmb_file=flags[\"entityEmb_file\"] if \"entityEmb_file\" in flags else None,\n",
    "        doc_size=flags[\"doc_size\"] if \"doc_size\" in flags else None,\n",
    "        word_size=flags[\"word_size\"] if \"word_size\" in flags else None,\n",
    "        entity_size=flags[\"entity_size\"] if \"entity_size\" in flags else None,\n",
    "        entity_dim=flags[\"entity_dim\"] if \"entity_dim\" in flags else None,\n",
    "        entity_embedding_method=flags[\"entity_embedding_method\"]\n",
    "        if \"entity_embedding_method\" in flags\n",
    "        else None,\n",
    "        transform=flags[\"transform\"] if \"transform\" in flags else None,\n",
    "        train_ratio=flags[\"train_ratio\"] if \"train_ratio\" in flags else None,\n",
    "        # model\n",
    "        dim=flags[\"dim\"] if \"dim\" in flags else None,\n",
    "        layer_sizes=flags[\"layer_sizes\"] if \"layer_sizes\" in flags else None,\n",
    "        cross_layer_sizes=flags[\"cross_layer_sizes\"]\n",
    "        if \"cross_layer_sizes\" in flags\n",
    "        else None,\n",
    "        cross_layers=flags[\"cross_layers\"] if \"cross_layers\" in flags else None,\n",
    "        activation=flags[\"activation\"] if \"activation\" in flags else None,\n",
    "        cross_activation=flags[\"cross_activation\"]\n",
    "        if \"cross_activation\" in flags\n",
    "        else \"identity\",\n",
    "        user_dropout=flags[\"user_dropout\"] if \"user_dropout\" in flags else False,\n",
    "        dropout=flags[\"dropout\"] if \"dropout\" in flags else [0.0],\n",
    "        attention_layer_sizes=flags[\"attention_layer_sizes\"]\n",
    "        if \"attention_layer_sizes\" in flags\n",
    "        else None,\n",
    "        attention_activation=flags[\"attention_activation\"]\n",
    "        if \"attention_activation\" in flags\n",
    "        else None,\n",
    "        attention_dropout=flags[\"attention_dropout\"]\n",
    "        if \"attention_dropout\" in flags\n",
    "        else 0.0,\n",
    "        model_type=flags[\"model_type\"] if \"model_type\" in flags else None,\n",
    "        method=flags[\"method\"] if \"method\" in flags else None,\n",
    "        load_saved_model=flags[\"load_saved_model\"]\n",
    "        if \"load_saved_model\" in flags\n",
    "        else False,\n",
    "        load_model_name=flags[\"load_model_name\"]\n",
    "        if \"load_model_name\" in flags\n",
    "        else None,\n",
    "        filter_sizes=flags[\"filter_sizes\"] if \"filter_sizes\" in flags else None,\n",
    "        num_filters=flags[\"num_filters\"] if \"num_filters\" in flags else None,\n",
    "        mu=flags[\"mu\"] if \"mu\" in flags else None,\n",
    "        fast_CIN_d=flags[\"fast_CIN_d\"] if \"fast_CIN_d\" in flags else 0,\n",
    "        use_Linear_part=flags[\"use_Linear_part\"]\n",
    "        if \"use_Linear_part\" in flags\n",
    "        else False,\n",
    "        use_FM_part=flags[\"use_FM_part\"] if \"use_FM_part\" in flags else False,\n",
    "        use_CIN_part=flags[\"use_CIN_part\"] if \"use_CIN_part\" in flags else False,\n",
    "        use_DNN_part=flags[\"use_DNN_part\"] if \"use_DNN_part\" in flags else False,\n",
    "        # train\n",
    "        init_method=flags[\"init_method\"] if \"init_method\" in flags else \"tnormal\",\n",
    "        init_value=flags[\"init_value\"] if \"init_value\" in flags else 0.01,\n",
    "        embed_l2=flags[\"embed_l2\"] if \"embed_l2\" in flags else 0.0000,\n",
    "        embed_l1=flags[\"embed_l1\"] if \"embed_l1\" in flags else 0.0000,\n",
    "        layer_l2=flags[\"layer_l2\"] if \"layer_l2\" in flags else 0.0000,\n",
    "        layer_l1=flags[\"layer_l1\"] if \"layer_l1\" in flags else 0.0000,\n",
    "        cross_l2=flags[\"cross_l2\"] if \"cross_l2\" in flags else 0.0000,\n",
    "        cross_l1=flags[\"cross_l1\"] if \"cross_l1\" in flags else 0.0000,\n",
    "        reg_kg=flags[\"reg_kg\"] if \"reg_kg\" in flags else 0.0000,\n",
    "        learning_rate=flags[\"learning_rate\"] if \"learning_rate\" in flags else 0.001,\n",
    "        lr_rs=flags[\"lr_rs\"] if \"lr_rs\" in flags else 1,\n",
    "        lr_kg=flags[\"lr_kg\"] if \"lr_kg\" in flags else 0.5,\n",
    "        kg_training_interval=flags[\"kg_training_interval\"]\n",
    "        if \"kg_training_interval\" in flags\n",
    "        else 5,\n",
    "        max_grad_norm=flags[\"max_grad_norm\"] if \"max_grad_norm\" in flags else 2,\n",
    "        is_clip_norm=flags[\"is_clip_norm\"] if \"is_clip_norm\" in flags else 0,\n",
    "        dtype=flags[\"dtype\"] if \"dtype\" in flags else 32,\n",
    "        loss=flags[\"loss\"] if \"loss\" in flags else None,\n",
    "        optimizer=flags[\"optimizer\"] if \"optimizer\" in flags else \"adam\",\n",
    "        epochs=flags[\"epochs\"] if \"epochs\" in flags else 10,\n",
    "        batch_size=flags[\"batch_size\"] if \"batch_size\" in flags else 1,\n",
    "        enable_BN=flags[\"enable_BN\"] if \"enable_BN\" in flags else False,\n",
    "        # show info\n",
    "        show_step=flags[\"show_step\"] if \"show_step\" in flags else 1,\n",
    "        save_model=flags[\"save_model\"] if \"save_model\" in flags else True,\n",
    "        save_epoch=flags[\"save_epoch\"] if \"save_epoch\" in flags else 5,\n",
    "        metrics=flags[\"metrics\"] if \"metrics\" in flags else None,\n",
    "        write_tfevents=flags[\"write_tfevents\"] if \"write_tfevents\" in flags else False,\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_hparams(yaml_file=None, **kwargs):\n",
    "    \"\"\"Prepare the model hyperparameters and check that all have the correct value.\n",
    "    Args:\n",
    "        yaml_file (str): YAML file as configuration.\n",
    "    Returns:\n",
    "        obj: Hyperparameter object in TF (tf.contrib.training.HParams).\n",
    "    \"\"\"\n",
    "    if yaml_file is not None:\n",
    "        config = load_yaml(yaml_file)\n",
    "        config = flat_config(config)\n",
    "    else:\n",
    "        config = {}\n",
    "\n",
    "    if kwargs:\n",
    "        for name, value in six.iteritems(kwargs):\n",
    "            config[name] = value\n",
    "\n",
    "    check_nn_config(config)\n",
    "    return create_hparams(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "from tempfile import TemporaryDirectory\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def maybe_download(url, filename=None, work_directory=\".\", expected_bytes=None):\n",
    "    \"\"\"Download a file if it is not already downloaded.\n",
    "    Args:\n",
    "        filename (str): File name.\n",
    "        work_directory (str): Working directory.\n",
    "        url (str): URL of the file to download.\n",
    "        expected_bytes (int): Expected file size in bytes.\n",
    "        \n",
    "    Returns:\n",
    "        str: File path of the file downloaded.\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        filename = url.split(\"/\")[-1]\n",
    "    os.makedirs(work_directory, exist_ok=True)\n",
    "    filepath = os.path.join(work_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "\n",
    "        r = requests.get(url, stream=True)\n",
    "        total_size = int(r.headers.get(\"content-length\", 0))\n",
    "        block_size = 1024\n",
    "        num_iterables = math.ceil(total_size / block_size)\n",
    "\n",
    "        with open(filepath, \"wb\") as file:\n",
    "            for data in tqdm(\n",
    "                r.iter_content(block_size),\n",
    "                total=num_iterables,\n",
    "                unit=\"KB\",\n",
    "                unit_scale=True,\n",
    "            ):\n",
    "                file.write(data)\n",
    "    else:\n",
    "        log.debug(\"File {} already downloaded\".format(filepath))\n",
    "    if expected_bytes is not None:\n",
    "        statinfo = os.stat(filepath)\n",
    "        if statinfo.st_size != expected_bytes:\n",
    "            os.remove(filepath)\n",
    "            raise IOError(\"Failed to verify {}\".format(filepath))\n",
    "\n",
    "    return filepath\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def download_path(path=None):\n",
    "    \"\"\"Return a path to download data. If `path=None`, then it yields a temporal path that is eventually deleted, \n",
    "    otherwise the real path of the input. \n",
    "    Args:\n",
    "        path (str): Path to download data.\n",
    "    Returns:\n",
    "        str: Real path where the data is stored.\n",
    "    Examples:\n",
    "        >>> with download_path() as path:\n",
    "        >>> ... maybe_download(url=\"http://example.com/file.zip\", work_directory=path)\n",
    "    \"\"\"\n",
    "    if path is None:\n",
    "        tmp_dir = TemporaryDirectory()\n",
    "        try:\n",
    "            yield tmp_dir.name\n",
    "        finally:\n",
    "            tmp_dir.cleanup()\n",
    "    else:\n",
    "        path = os.path.realpath(path)\n",
    "        yield path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def download_deeprec_resources(azure_container_url, data_path, remote_resource_name):\n",
    "    \"\"\"Download resources.\n",
    "    Args:\n",
    "        azure_container_url (str): URL of Azure container.\n",
    "        data_path (str): Path to download the resources.\n",
    "        remote_resource_name (str): Name of the resource.\n",
    "    \"\"\"\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "    remote_path = azure_container_url + remote_resource_name\n",
    "    maybe_download(remote_path, remote_resource_name, data_path)\n",
    "    zip_ref = zipfile.ZipFile(os.path.join(data_path, remote_resource_name), \"r\")\n",
    "    zip_ref.extractall(data_path)\n",
    "    zip_ref.close()\n",
    "    os.remove(os.path.join(data_path, remote_resource_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10.3k/10.3k [00:03<00:00, 3.24kKB/s]\n"
     ]
    }
   ],
   "source": [
    "tmpdir = TemporaryDirectory()\n",
    "data_path = tmpdir.name\n",
    "yaml_file = os.path.join(data_path, r'xDeepFM.yaml')\n",
    "if not os.path.exists(yaml_file):\n",
    "    download_deeprec_resources(r'https://recodatasets.blob.core.windows.net/deeprec/', data_path, 'xdeepfmresources.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = prepare_hparams(yaml_file,\n",
    "                          FEATURE_COUNT=1703, \n",
    "                          FIELD_COUNT=25, \n",
    "                          cross_l2=0.001, \n",
    "                          embed_l2=0.001, \n",
    "                          layer_l2=0.001,\n",
    "                          learning_rate=0.001, \n",
    "                          dropout = [0.5, 0.5],\n",
    "                          batch_size=BATCH_SIZE_CRITEO, \n",
    "                          epochs=EPOCHS_FOR_CRITEO_RUN, \n",
    "                          cross_layer_sizes=[20, 10], \n",
    "                          init_value=0.1, \n",
    "                          layer_sizes=[20,20],\n",
    "                          use_Linear_part=True, \n",
    "                          use_CIN_part=True, \n",
    "                          use_DNN_part=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "codeCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseIterator(object):\n",
    "    @abc.abstractmethod\n",
    "    def parser_one_line(self, line):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def load_data_from_file(self, infile):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def _convert_data(self, labels, features):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def gen_feed_dict(self, data_dict):\n",
    "        pass\n",
    "class FFMTextIterator(BaseIterator):\n",
    "    \"\"\"Data loader for FFM format based models, such as xDeepFM.\n",
    "    Iterator will not load the whole data into memory. Instead, it loads data into memory\n",
    "    per mini-batch, so that large files can be used as input data.\n",
    "    \"\"\"\n",
    "    def __init__(self, hparams, graph, col_spliter=\" \", ID_spliter=\"%\"):\n",
    "        \"\"\"Initialize an iterator. Create necessary placeholders for the model.\n",
    "        \n",
    "        Args:\n",
    "            hparams (obj): Global hyper-parameters. Some key settings such as #_feature and #_field are there.\n",
    "            graph (obj): the running graph. All created placeholder will be added to this graph.\n",
    "            col_spliter (str): column splitter in one line.\n",
    "            ID_spliter (str): ID splitter in one line.\n",
    "        \"\"\"\n",
    "        self.feature_cnt = hparams.FEATURE_COUNT\n",
    "        self.field_cnt = hparams.FIELD_COUNT\n",
    "        self.col_spliter = col_spliter\n",
    "        self.ID_spliter = ID_spliter\n",
    "        self.batch_size = hparams.batch_size\n",
    "\n",
    "        self.graph = graph\n",
    "        with self.graph.as_default():\n",
    "            self.labels = tf.placeholder(tf.float32, [None, 1], name=\"label\")\n",
    "            self.fm_feat_indices = tf.placeholder(\n",
    "                tf.int64, [None, 2], name=\"fm_feat_indices\"\n",
    "            )\n",
    "            self.fm_feat_values = tf.placeholder(\n",
    "                tf.float32, [None], name=\"fm_feat_values\"\n",
    "            )\n",
    "            self.fm_feat_shape = tf.placeholder(tf.int64, [None], name=\"fm_feat_shape\")\n",
    "            self.dnn_feat_indices = tf.placeholder(\n",
    "                tf.int64, [None, 2], name=\"dnn_feat_indices\"\n",
    "            )\n",
    "            self.dnn_feat_values = tf.placeholder(\n",
    "                tf.int64, [None], name=\"dnn_feat_values\"\n",
    "            )\n",
    "            self.dnn_feat_weights = tf.placeholder(\n",
    "                tf.float32, [None], name=\"dnn_feat_weights\"\n",
    "            )\n",
    "            self.dnn_feat_shape = tf.placeholder(\n",
    "                tf.int64, [None], name=\"dnn_feat_shape\"\n",
    "            )\n",
    "\n",
    "    def parser_one_line(self, line):\n",
    "        \"\"\"Parse one string line into feature values.\n",
    "        \n",
    "        Args:\n",
    "            line (str): a string indicating one instance\n",
    "        Returns:\n",
    "            list: Parsed results,including label, features and impression_id\n",
    "        \"\"\"\n",
    "        impression_id = None\n",
    "        words = line.strip().split(self.ID_spliter)\n",
    "        if len(words) == 2:\n",
    "            impression_id = words[1].strip()\n",
    "\n",
    "        cols = words[0].strip().split(self.col_spliter)\n",
    "\n",
    "        label = float(cols[0])\n",
    "\n",
    "        features = []\n",
    "        for word in cols[1:]:\n",
    "            if not word.strip():\n",
    "                continue\n",
    "            tokens = word.split(\":\")\n",
    "            features.append([int(tokens[0]) - 1, int(tokens[1]) - 1, float(tokens[2])])\n",
    "\n",
    "        return label, features, impression_id\n",
    "\n",
    "    def load_data_from_file(self, infile):\n",
    "        \"\"\"Read and parse data from a file.\n",
    "        \n",
    "        Args:\n",
    "            infile (str): text input file. Each line in this file is an instance.\n",
    "        Returns:\n",
    "            obj: An iterator that will yields parsed results, in the format of graph feed_dict.\n",
    "        \"\"\"\n",
    "        label_list = []\n",
    "        features_list = []\n",
    "        impression_id_list = []\n",
    "        cnt = 0\n",
    "\n",
    "        with tf.gfile.GFile(infile, \"r\") as rd:\n",
    "            while True:\n",
    "                line = rd.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "\n",
    "                label, features, impression_id = self.parser_one_line(line)\n",
    "\n",
    "                features_list.append(features)\n",
    "                label_list.append(label)\n",
    "                impression_id_list.append(impression_id)\n",
    "\n",
    "                cnt += 1\n",
    "                if cnt == self.batch_size:\n",
    "                    res = self._convert_data(label_list, features_list)\n",
    "                    yield self.gen_feed_dict(res)\n",
    "                    label_list = []\n",
    "                    features_list = []\n",
    "                    impression_id_list = []\n",
    "                    cnt = 0\n",
    "            if cnt > 0:\n",
    "                res = self._convert_data(label_list, features_list)\n",
    "                yield self.gen_feed_dict(res)\n",
    "\n",
    "    def _convert_data(self, labels, features):\n",
    "        \"\"\"Convert data into numpy arrays that are good for further operation.\n",
    "        \n",
    "        Args:\n",
    "            labels (list): a list of ground-truth labels.\n",
    "            features (list): a 3-dimensional list, carrying a list (batch_size) of feature array,\n",
    "                    where each feature array is a list of [field_idx, feature_idx, feature_value] tuple.\n",
    "        Returns:\n",
    "            dict: A dictionary, contains multiple numpy arrays that are convenient for further operation.\n",
    "        \"\"\"\n",
    "        dim = self.feature_cnt\n",
    "        FIELD_COUNT = self.field_cnt\n",
    "        instance_cnt = len(labels)\n",
    "\n",
    "        fm_feat_indices = []\n",
    "        fm_feat_values = []\n",
    "        fm_feat_shape = [instance_cnt, dim]\n",
    "\n",
    "        dnn_feat_indices = []\n",
    "        dnn_feat_values = []\n",
    "        dnn_feat_weights = []\n",
    "        dnn_feat_shape = [instance_cnt * FIELD_COUNT, -1]\n",
    "\n",
    "        for i in range(instance_cnt):\n",
    "            m = len(features[i])\n",
    "            dnn_feat_dic = {}\n",
    "            for j in range(m):\n",
    "                fm_feat_indices.append([i, features[i][j][1]])\n",
    "                fm_feat_values.append(features[i][j][2])\n",
    "                if features[i][j][0] not in dnn_feat_dic:\n",
    "                    dnn_feat_dic[features[i][j][0]] = 0\n",
    "                else:\n",
    "                    dnn_feat_dic[features[i][j][0]] += 1\n",
    "                dnn_feat_indices.append(\n",
    "                    [\n",
    "                        i * FIELD_COUNT + features[i][j][0],\n",
    "                        dnn_feat_dic[features[i][j][0]],\n",
    "                    ]\n",
    "                )\n",
    "                dnn_feat_values.append(features[i][j][1])\n",
    "                dnn_feat_weights.append(features[i][j][2])\n",
    "                if dnn_feat_shape[1] < dnn_feat_dic[features[i][j][0]]:\n",
    "                    dnn_feat_shape[1] = dnn_feat_dic[features[i][j][0]]\n",
    "        dnn_feat_shape[1] += 1\n",
    "\n",
    "        sorted_index = sorted(\n",
    "            range(len(dnn_feat_indices)),\n",
    "            key=lambda k: (dnn_feat_indices[k][0], dnn_feat_indices[k][1]),\n",
    "        )\n",
    "\n",
    "        res = {}\n",
    "        res[\"fm_feat_indices\"] = np.asarray(fm_feat_indices, dtype=np.int64)\n",
    "        res[\"fm_feat_values\"] = np.asarray(fm_feat_values, dtype=np.float32)\n",
    "        res[\"fm_feat_shape\"] = np.asarray(fm_feat_shape, dtype=np.int64)\n",
    "        res[\"labels\"] = np.asarray([[label] for label in labels], dtype=np.float32)\n",
    "\n",
    "        res[\"dnn_feat_indices\"] = np.asarray(dnn_feat_indices, dtype=np.int64)[\n",
    "            sorted_index\n",
    "        ]\n",
    "        res[\"dnn_feat_values\"] = np.asarray(dnn_feat_values, dtype=np.int64)[\n",
    "            sorted_index\n",
    "        ]\n",
    "        res[\"dnn_feat_weights\"] = np.asarray(dnn_feat_weights, dtype=np.float32)[\n",
    "            sorted_index\n",
    "        ]\n",
    "        res[\"dnn_feat_shape\"] = np.asarray(dnn_feat_shape, dtype=np.int64)\n",
    "        return res\n",
    "\n",
    "    def gen_feed_dict(self, data_dict):\n",
    "        \"\"\"Construct a dictionary that maps graph elements to values.\n",
    "        Args:\n",
    "            data_dict (dict): a dictionary that maps string name to numpy arrays.\n",
    "        Returns:\n",
    "            dict: a dictionary that maps graph elements to numpy arrays.\n",
    "        \"\"\"\n",
    "        feed_dict = {\n",
    "            self.labels: data_dict[\"labels\"],\n",
    "            self.fm_feat_indices: data_dict[\"fm_feat_indices\"],\n",
    "            self.fm_feat_values: data_dict[\"fm_feat_values\"],\n",
    "            self.fm_feat_shape: data_dict[\"fm_feat_shape\"],\n",
    "            self.dnn_feat_indices: data_dict[\"dnn_feat_indices\"],\n",
    "            self.dnn_feat_values: data_dict[\"dnn_feat_values\"],\n",
    "            self.dnn_feat_weights: data_dict[\"dnn_feat_weights\"],\n",
    "            self.dnn_feat_shape: data_dict[\"dnn_feat_shape\"],\n",
    "        }\n",
    "        return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(\"df_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['os', 'browser','slot_id', \"ssp_uid\",'carrier_name', 'region','creative_type','sponsor_id','campaign_id','creative_id',\"is_rt\", 'request_hour',\n",
    "       'request_month', 'request_day', 'request_dayofweek', 'rt_times']\n",
    "for c in cat_cols:\n",
    "    vv, obj = pd.factorize(df_final[c])\n",
    "    df_final[c] = vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.drop([\"Unnamed: 0\"], axis = 1)\n",
    "df_final = df_final.drop([\"request_time\"], axis = 1)\n",
    "df_final = df_final.drop([\"click_time\"], axis = 1)\n",
    "df_final = df_final.drop([\"cv_time\"], axis = 1)\n",
    "df_final = df_final.drop([\"cv_flg\"], axis = 1)\n",
    "df_final = df_final.drop([\"ssp_uid\"], axis = 1)\n",
    "df_final = df_final.drop([\"req_id\"], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = df_final.drop([\"click_flg\"], axis = 1)\n",
    "num_data=len(df_final)\n",
    "train = f.iloc[0:int(num_data*0.8),:]\n",
    "val = f.iloc[int(num_data*0.8):int(num_data*0.9),:]\n",
    "test = f.iloc[int(num_data*0.9):num_data,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = df_final.loc[int(num_data*0.9):num_data,\"click_flg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(train)):\n",
    "  raw = df_final.iloc[row,:]\n",
    "  text = str(raw[\"click_flg\"])\n",
    "  text = text + \" \"\n",
    "  feature_idx_past = 1\n",
    "  for i, name in enumerate(train.columns):\n",
    "    feature_n = i+1\n",
    "    text = text + str(feature_n) + \":\"\n",
    "    if name in ['os', 'browser','slot_id','carrier_name', 'region','creative_type','sponsor_id','campaign_id','creative_id',\"is_rt\",'request_hour',\n",
    "       'request_month', 'request_day', 'request_dayofweek', 'rt_times']:\n",
    "      a = df_final[name].unique()\n",
    "      idx = np.where(a==raw[name])[0]\n",
    "      feature_idx = int(idx) + feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + len(a)\n",
    "      feature_value = 1\n",
    "    else:\n",
    "      feature_idx = feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + 1\n",
    "      feature_value = raw[name]\n",
    "    text = text + str(int(feature_idx)) + \":\"\n",
    "    text = text + str(feature_value) + \" \"\n",
    "  with open('save_1.txt', 'a') as rd:\n",
    "    print(text, file=rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(val)):\n",
    "  raw = df_final.iloc[row+160000,:]\n",
    "  text = str(raw[\"click_flg\"])\n",
    "  text = text + \" \"\n",
    "  feature_idx_past = 1\n",
    "  for i, name in enumerate(val.columns):\n",
    "    feature_n = i+1\n",
    "    text = text + str(feature_n) + \":\"\n",
    "    if name in ['os', 'browser','slot_id','carrier_name', 'region','creative_type','sponsor_id','campaign_id','creative_id',\"is_rt\",'request_hour',\n",
    "       'request_month', 'request_day', 'request_dayofweek', 'rt_times']:\n",
    "      a = df_final[name].unique()\n",
    "      idx = np.where(a==raw[name])[0]\n",
    "      feature_idx = int(idx) + feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + len(a)\n",
    "      feature_value = 1\n",
    "    else:\n",
    "      feature_idx = feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + 1\n",
    "      feature_value = raw[name]\n",
    "    text = text + str(int(feature_idx)) + \":\"\n",
    "    text = text + str(feature_value) + \" \"\n",
    "  text = text \n",
    "  with open('save_2.txt', 'a') as rd:\n",
    "    print(text, file=rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in range(len(test)):\n",
    "  raw = df_final.iloc[row+180000,:]\n",
    "  text = str(raw[\"click_flg\"])\n",
    "  text = text + \" \"\n",
    "  feature_idx_past = 1\n",
    "  for i, name in enumerate(test.columns):\n",
    "    feature_n = i+1\n",
    "    text = text + str(feature_n) + \":\"\n",
    "    if name in ['os', 'browser','slot_id','carrier_name', 'region','creative_type','sponsor_id','campaign_id','creative_id',\"is_rt\",'request_hour',\n",
    "       'request_month', 'request_day', 'request_dayofweek', 'rt_times']:\n",
    "      a = df_final[name].unique()\n",
    "      idx = np.where(a==raw[name])[0]\n",
    "      feature_idx = int(idx) + feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + len(a)\n",
    "      feature_value = 1\n",
    "    else:\n",
    "      feature_idx = feature_idx_past\n",
    "      feature_idx_past = feature_idx_past + 1\n",
    "      feature_value = raw[name]\n",
    "    text = text + str(int(feature_idx)) + \":\"\n",
    "    text = text + str(feature_value) + \" \"\n",
    "  text = text \n",
    "  with open('save_3.txt', 'a') as rd:\n",
    "    print(text, file=rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.gfile.GFile(\"save_1.txt\", \"r\") as rd:\n",
    "  count = 0\n",
    "  while True:\n",
    "    line = rd.readline()\n",
    "    print(line)\n",
    "    count += 1\n",
    "    if count == 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add linear part.\n",
      "Add CIN part.\n",
      "Add DNN part.\n"
     ]
    }
   ],
   "source": [
    "input_creator = FFMTextIterator\n",
    "model = XDeepFMModel(hparams, input_creator, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch 1 train info: auc:0.5813, logloss:0.357 eval info: auc:0.5919, logloss:0.3648\n",
      "at epoch 1 , train time: 144.7 eval time: 105.6\n",
      "at epoch 2 train info: auc:0.6998, logloss:0.3054 eval info: auc:0.6838, logloss:0.3166\n",
      "at epoch 2 , train time: 143.9 eval time: 105.5\n",
      "at epoch 3 train info: auc:0.7664, logloss:0.2852 eval info: auc:0.7403, logloss:0.2989\n",
      "at epoch 3 , train time: 143.8 eval time: 105.5\n",
      "at epoch 4 train info: auc:0.8023, logloss:0.2702 eval info: auc:0.7746, logloss:0.2869\n",
      "at epoch 4 , train time: 144.3 eval time: 105.6\n",
      "at epoch 5 train info: auc:0.8163, logloss:0.2632 eval info: auc:0.7862, logloss:0.2827\n",
      "at epoch 5 , train time: 144.1 eval time: 105.6\n",
      "at epoch 6 train info: auc:0.8216, logloss:0.2603 eval info: auc:0.789, logloss:0.2827\n",
      "at epoch 6 , train time: 144.2 eval time: 105.6\n",
      "at epoch 7 train info: auc:0.8249, logloss:0.2588 eval info: auc:0.7898, logloss:0.2838\n",
      "at epoch 7 , train time: 143.7 eval time: 105.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.XDeepFMModel at 0x7fdeeac7b470>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit('save_1.txt', 'save_2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdeep_pred = model.predict('save_3.txt','output_5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_4.txt', 'r') as rd:\n",
    "    xdeep_pred = [float(s.strip()) for s in rd.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7870081332370666\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(test_y, xdeep_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/matplotlib/font_manager.py:1320: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde95f42da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# precision-recall curve and f1\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from matplotlib import pyplot\n",
    "# generate a no skill prediction (majority class)\n",
    "no_skill_probs = [i for i in np.random.rand(len(test_y))]\n",
    "# calculate precision and recall for each threshold\n",
    "ns_precision, ns_recall, _ = precision_recall_curve(test_y, no_skill_probs)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(test_y, xdeep_pred)\n",
    "\n",
    "# plot the precision-recall curves\n",
    "pyplot.plot(ns_recall, ns_precision, linestyle='--', label='No Skill')\n",
    "pyplot.plot(lr_recall, lr_precision, linestyle='-', label='Logistic')\n",
    "# axis labels\n",
    "pyplot.xlabel('Recall')\n",
    "pyplot.ylabel('Precision')\n",
    "# show the legend\n",
    "pyplot.legend()\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC(PR):0.31031180298701566\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import auc\n",
    "print(\"AUC(PR):%s\" %str(auc(lr_recall, lr_precision)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
